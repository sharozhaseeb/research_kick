{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \n",
    "You ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \n",
    "\n",
    "<First Step>\n",
    "Your task is to lightly interview the user;\n",
    "\n",
    "Start with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \n",
    "Ask one question per turn. \n",
    "Don't ask more than 5 questions in total.\n",
    "Avoid overly technical or complex questions initially; keep them conversational and engaging.\n",
    "Let the conversation flow naturally to build trust and comfort.\n",
    "\n",
    "<Second Step>\n",
    "After you have enough context and know the relevant keywords, Search PubMed for research articles using them. \n",
    "\n",
    "<Third Step>\n",
    "After you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\n",
    "Also add the names of the articles as sources at the end of the response.\n",
    "\n",
    "<Forth Step>\n",
    "Generate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pymed import PubMed\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Check your .env file.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def pubmed_search(query, max_results=5):\n",
    "    pubmed = PubMed(tool=\"MyTool\", email=\"sharozhaseeb1@gmail.com\")\n",
    "    results = pubmed.query(query, max_results=max_results)\n",
    "\n",
    "    result_list = []\n",
    "    for article in results:\n",
    "        child_dict = {}\n",
    "        child_dict['title'] = article.title\n",
    "        child_dict['authors'] = [((author['lastname'] or '') + ', ' + (author['firstname'] or '')).strip(', ') for author in article.authors]\n",
    "        child_dict['abstract'] = article.abstract\n",
    "        child_dict['publication_date'] = article.publication_date\n",
    "        result_list.append(child_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"pubmed_search\",\n",
    "        \"description\": \"Searches PubMed for research articles based on a given query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The keyword to search for.\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The maximum number of results to retrieve.\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I work in machine learning specifically transformers, can you get me some research ideas?\"}]\n",
    "            # {\"role\": \"user\", \"content\": \"Hi, who are you ?\"}]\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    # tool_choice=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_euOxD1SrpSINfZ6O8fzSWs5J', function=Function(arguments='{\"query\":\"transformers machine learning\",\"max_results\":5}', name='pubmed_search'), type='function')], annotations=[]))\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool_calls'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].finish_reason\n",
    "\n",
    "#output for tool call will be 'tool_calls'\n",
    "#output for chat will be 'stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_euOxD1SrpSINfZ6O8fzSWs5J', 'function': {'arguments': '{\"query\":\"transformers machine learning\",\"max_results\":5}', 'name': 'pubmed_search'}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "tool_calls = completion.choices[0].message.tool_calls\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    print(tool_call.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = completion.choices[0].message.tool_calls[0]\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "result = pubmed_search(args[\"query\"], args[\"max_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Searching PubMed for articles...\n",
      "Query: creatine supplementation\n",
      "Max Results: 5\n",
      "--------------------------------------------\n",
      "[{'title': '', 'authors': ['Guo, Minghan', 'Zhao, Lina', 'Cao, Li', 'Li, Xuan', 'Zhang, Jie', 'Dong, Yao', 'Wu, Ying', 'Gu, Shaobin'], 'abstract': 'Adequate protein consumption is essential for optimal physical fitness and enhancing athletic performance. This study explored the impact of ', 'publication_date': datetime.date(2025, 3, 13)}, {'title': 'Are there Effective Vegan-Friendly Supplements for Optimizing Health and Sports Performance? a Narrative Review.', 'authors': ['Nieto, Álvaro Vergara A', 'Diaz, Andrés Halabi', 'Hernández, Millaray'], 'abstract': 'Veganism, characterized by the exclusion of all animal-derived products, has grown in popularity due to ethical, environmental, and health considerations. However, vegan athletes often face unique nutritional challenges related to dietary deficiencies of critical nutrients such as proteins, vitamin B12, iron, calcium, and omega-3 fatty acids, among others. This narrative review aims to explore the efficacy and benefits of vegan-friendly supplements specifically tailored to athletic performance, focusing on essential micronutrients, ergogenic aids, and nutrient bioavailability.\\nNineteen key supplements are discussed, including protein powders, creatine, beta-alanine, caffeine, vitamin B12, vitamin D, omega-3 fatty acids, zinc, calcium, iron, iodine, vitamin K2, selenium, probiotics, nitrates, electrolytes (including sodium and potassium), taurine, vitamin A, and magnesium. Evidence suggests that the integration of these supplements into personalized nutrition plans can bridge dietary gaps while addressing specific performance needs, potentially leveling the competitive field for vegan athletes. Recent studies also highlight research gaps in sex-specific needs, synergistic effects, and strategies to enhance the bioavailability of nutrients from whole foods. Vegan diets, while conferring various benefits, require careful consideration of nutrient intake for athletes seeking optimal performance. Personalized biochemical assessments should be considered when possible for tailoring specific nutritional guidelines for each case. This narrative review provides practical guidelines for clinicians, nutritionists, trainers, sports scientists, and athletes to design personalized supplementation strategies that address common nutritional shortfalls, enhance performance, and serve as a foundation for future research in vegan sports nutrition.', 'publication_date': datetime.date(2025, 3, 12)}, {'title': 'Study on the Effect of Bifidobacterium adolescentis CCFM1066 on Exercise Performance, Gut Microbiota, and Its Metabolites in Mice.', 'authors': ['Wang, Hongchao', 'Ma, Huizi', 'Yan, Huimin', 'Pei, Zhangming', 'Zhao, Jianxin', 'Zhang, Hao', 'Zhang, Zhijian', 'Lu, Wenwei'], 'abstract': 'Prolonged high-intensity exercise consumes significant energy, leading to fatigue and decreased performance. This study explores the effects of Bifidobacterium adolescentis CCFM1066 on exercise performance, gut microbiota, and its metabolites in mice. The results of the mouse experiments showed the mice which were intervened by Bifidobacterium adolescentis CCFM1066 have a significant increase in exercise performance, including forceful swimming time, fatigue baton turning time, and forelimb grip strength. Through metagenomic sequencing and differential metabolites, analysis indicated that the intervention of CCFM1066 increased Lachnospiraceae bacterium, Parabacteroides goldsteinii, Bacteroides xylanisolvens, and Bifidobacterium adolescentis and altered the key metabolic pathways including protein digestion and absorption and biosynthesis of amino acids. Supplementation with CCFM1066 modulates the production of short-chain fatty acids (SCFAs) and fatty acid amides (FAAs) by gut microbiota, decreasing levels of lactic acid (LA), blood urea nitrogen (BUN), lactate dehydrogenase (LDH), and creatine kinase (CK) while increasing muscle and hepatic glycogen content, thus reducing central nervous system fatigue and thereby improving exercise endurance and performance. These findings provide new insights into nutritional interventions for sports performance.', 'publication_date': datetime.date(2025, 3, 11)}, {'title': 'Unraveling an Uncommon Encounter: Hypokalemic Periodic Paralysis with Brugada Phenocopy Amidst Hypokalemia.', 'authors': ['Dankar, Razan', 'Barakat, Salim', 'El-Charabaty, Elie', 'Hashmi, Syed Salman Hamid', 'El Sayegh, Suzanne E'], 'abstract': 'Hypokalemic periodic paralysis (HPP) presents a diagnostic challenge due to the painless muscle weakness it causes. This case discusses a patient with HPP along with electrocardiogram (ECG) findings of Brugada phenocopies (BrP) in the setting of hypokalemia. A review of the literature showed that it is the seventh documented example of BrP induced by hypokalemia alongside HPP.\\nA 43-year-old man presented to the emergency department with lower limb weakness. He attributed his symptoms to a substantial meal consumed after breaking his Ramadan fast, recalling a similar episode following heavy meals in the past. The patient was alert and oriented but demonstrated reduced strength in both upper and lower limbs. ECG revealed a Brugada type 1 pattern. Laboratory analysis revealed hypokalemia (2.5 mmol/l), elevated creatine kinase (326 U/l), and normal thyroid function. Following potassium supplementation, his symptoms resolved, and his ECG normalized.\\nHPP occurs in the context of increased carbohydrate intake, potentially leading to rapid insulin release and activation of Na-K ATPase, enhancing cellular potassium absorption and lowering serum potassium levels. Symptoms range from weakness and fatigue to severe neuromuscular weakness and cardiac arrhythmias. Investigating hypokalemia requires excluding hypomagnesemia, thyroid function tests, and metabolic acidosis/alkalosis before considering HPP. Management involves gradual oral potassium repletion to avoid the risk of hyperkalemia associated with intravenous administration.\\nClinicians should consider including HPP in differential diagnoses of patients presenting with weakness. In this case, electrophysiological evaluation suggested Brugada pattern induced by hypokalemia, which resolved with potassium supplementation.\\nThis case highlights the rarity and diagnostic challenges of hypokalemic periodic paralysis, offering critical insights into recognizing and managing such conditions in clinical practice.The case also demonstrates the importance of identifying reversible electrocardiogram changes like Brugada patterns, aiding in differentiation from persistent arrhythmias and avoiding unnecessary interventions.', 'publication_date': datetime.date(2025, 3, 7)}, {'title': 'D-BHB supplementation before moderate-intensity exercise suppresses lipolysis and selectively blunts exercise-induced long-chain acylcarnitine increase in pilot study of patients with long-chain fatty acid oxidation disorders.', 'authors': ['Gregor, Ashley N', 'Delerive, Philippe', 'Cuenoud, Bernard', 'Monnard, Irina', 'Redeuil, Karine', 'Harding, Cary O', 'Gillingham, Melanie B'], 'abstract': 'Patients with long-chain fatty acid oxidation disorders (LC-FAOD) have impaired endogenous ketone production due to defects in the beta-oxidation pathway. We explored supplementation of exogenous D-beta-hydroxybutyrate (D-BHB) as an alternative source of energy in a randomized, double-blinded crossover pilot study. Participants ≥18\\xa0years of age with a diagnosis of LC-FAOD completed two moderate-intensity treadmill exercises following an oral supplementation of D-BHB salts or an isocaloric maltodextrin beverage. Five subjects (1 VLCADD, 2 CPT2D, 2 LCHADD), 60\\xa0% male, mean age\\xa0=\\xa033\\xa0years were enrolled. Mild to moderate GI symptoms were related to ingestion of D-BHB. Plasma D-BHB was increased after oral D-BHB compared to maltodextrin (p\\xa0<\\xa0.001) with an average concentration of 0.43\\xa0mM in the post-exercise period. During exercise, free fatty acids (p\\xa0=\\xa0.01), fold change in long-chain acylcarnitine species (LC-AC) (p\\xa0≤\\xa0.03) and systolic BP (p\\xa0=\\xa0.02) were lower after D-BHB compared to the maltodextrin beverage. D-BHB suppresses lipolysis and selectively blunts exercise-induced long-chain acylcarnitines. There were no differences between beverages in acetylcarnitine, blood glucose, creatine kinase, VO2, HR, RPE or respiratory exchange ratio. Consumption of the D-BHB beverage was safe and well-tolerated. Plasma D-BHB levels achieved mild ketosis and suppressed lipolysis and the associated rise in LC-AC, but fell short of stimulating the energetic effects that might have resulted in altered exercise parameters such as RER, or HR. In conclusion, our results provide a strong rationale for future studies aimed toward defining the optimal multiple-dose regimen of D-BHB per day that might improve exercise tolerance and understanding the long-term impact of treatment in LC-FAOD subjects.', 'publication_date': datetime.date(2025, 3, 7)}]\n"
     ]
    }
   ],
   "source": [
    "# from helper import pubmed_search\n",
    "# print(pubmed_search(\"creatine supplementation\", max_results=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Accelerating the inference of string generation-based chemical reaction models for industrial applications.', 'authors': ['Andronov, Mikhail', 'Andronova, Natalia', 'Wand, Michael', 'Schmidhuber, Jürgen', 'Clevert, Djork-Arné'], 'abstract': \"Transformer-based, template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest to computer-aided synthesis planning systems, as they offer state-of-the-art accuracy. However, their slow inference speed limits their practical utility in such applications. To address this challenge, we propose speculative decoding with a simple chemically specific drafting strategy and apply it to the Molecular Transformer, an encoder-decoder transformer for conditional SMILES generation. Our approach achieves over 3X faster inference in reaction product prediction and single-step retrosynthesis with no loss in accuracy, increasing the potential of the transformer as the backbone of synthesis planning systems. To accelerate the simultaneous generation of multiple precursor SMILES for a given query SMILES in single-step retrosynthesis, we introduce Speculative Beam Search, a novel algorithm tackling the challenge of beam search acceleration with speculative decoding. Our methods aim to improve transformer-based models' scalability and industrial applicability in synthesis planning.\", 'publication_date': datetime.date(2025, 3, 11)}, {'title': 'Principal component analysis and fine-tuned vision transformation integrating model explainability for breast cancer prediction.', 'authors': ['Luong, Huong Hoang', 'Hong, Phuc Phan', 'Minh, Dat Vo', 'Quang, Thinh Nguyen Le', 'The, Anh Dinh', 'Thai-Nghe, Nguyen', 'Nguyen, Hai Thanh'], 'abstract': 'Breast cancer, which is the most commonly diagnosed cancers among women, is a notable health issues globally. Breast cancer is a result of abnormal cells in the breast tissue growing out of control. Histopathology, which refers to the detection and learning of tissue diseases, has appeared as a solution for breast cancer treatment as it plays a vital role in its diagnosis and classification. Thus, considerable research on histopathology in medical and computer science has been conducted to develop an effective method for breast cancer treatment. In this study, a vision Transformer (ViT) was employed to classify tumors into two classes, benign and malignant, in the Breast Cancer Histopathological Database (BreakHis). To enhance the model performance, we introduced the novel multi-head locality large kernel self-attention during fine-tuning, achieving an accuracy of 95.94% at 100× magnification, thereby improving the accuracy by 3.34% compared to a standard ViT (which uses multi-head self-attention). In addition, the application of principal component analysis for dimensionality reduction led to an accuracy improvement of 3.34%, highlighting its role in mitigating overfitting and reducing the computational complexity. In the final phase, SHapley Additive exPlanations, Local Interpretable Model-agnostic Explanations, and Gradient-weighted Class Activation Mapping were used for the interpretability and explainability of machine-learning models, aiding in understanding the feature importance and local explanations, and visualizing the model attention. In another experiment, ensemble learning with VGGIN further boosted the performance to 97.13% accuracy. Our approach exhibited a 0.98% to 17.13% improvement in accuracy compared with state-of-the-art methods, establishing a new benchmark for breast cancer histopathological image classification.', 'publication_date': datetime.date(2025, 3, 10)}, {'title': \"A novel integrative multimodal classifier to enhance the diagnosis of Parkinson's disease.\", 'authors': ['Zhou, Xiaoyan', 'Parisi, Luca', 'Huang, Wentao', 'Zhang, Yihan', 'Huang, Xiaoqun', 'Youseffi, Mansour', 'Javid, Farideh', 'Ma, Renfei'], 'abstract': \"Parkinson's disease (PD) is a complex, progressive neurodegenerative disorder with high heterogeneity, making early diagnosis difficult. Early detection and intervention are crucial for slowing PD progression. Understanding PD's diverse pathways and mechanisms is key to advancing knowledge. Recent advances in noninvasive imaging and multi-omics technologies have provided valuable insights into PD's underlying causes and biological processes. However, integrating these diverse data sources remains challenging, especially when deriving meaningful low-level features that can serve as diagnostic indicators. This study developed and validated a novel integrative, multimodal predictive model for detecting PD based on features derived from multimodal data, including hematological information, proteomics, RNA sequencing, metabolomics, and dopamine transporter scan imaging, sourced from the Parkinson's Progression Markers Initiative. Several model architectures were investigated and evaluated, including support vector machine, eXtreme Gradient Boosting, fully connected neural networks with concatenation and joint modeling (FCNN_C and FCNN_JM), and a multimodal encoder-based model with multi-head cross-attention (MMT_CA). The MMT_CA model demonstrated superior predictive performance, achieving a balanced classification accuracy of 97.7%, thus highlighting its ability to capture and leverage cross-modality inter-dependencies to aid predictive analytics. Furthermore, feature importance analysis using SHapley Additive exPlanations not only identified crucial diagnostic biomarkers to inform the predictive models in this study but also holds potential for future research aimed at integrated functional analyses of PD from a multi-omics perspective, ultimately revealing targets required for precision medicine approaches to aid treatment of PD aimed at slowing down its progression.\", 'publication_date': datetime.date(2025, 3, 10)}, {'title': 'Non-invasive enhanced hypertension detection through ballistocardiograph signals with Mamba model.', 'authors': ['Alhudhaif, Adi', 'Polat, Kemal'], 'abstract': \"This study explores using ballistocardiography (BCG), a non-invasive cardiovascular monitoring technique, combined with advanced machine learning and deep learning models for hypertension detection. The motivation behind this research is to develop a non-invasive and efficient approach for long-term hypertension monitoring, facilitating home-based health assessments. A dataset of 128 BCG recordings has been used, capturing body micro-vibrations from cardiac activity. Various classification models, including Mamba Classifier, Transformer, Stacking, Voting, and XGBoost, were applied to differentiate hypertensive individuals from normotensive ones. In this study, integrating BCG signals with deep learning and machine learning models for hypertension detection is distinguished from previous literature by employing the Mamba deep learning architecture and Transformer-based models. Unlike conventional methods in literature, this study enables more effective analysis of time-series data with the Mamba architecture, capturing long-term signal dependencies and achieving higher accuracy rates. In particular, the combined use of Mamba architecture and the Transformer model's signal processing capabilities represents a novel approach not previously seen in the literature. While existing studies on BCG signals typically rely on traditional machine learning algorithms, this study aims to achieve higher success rates in hypertension detection by integrating signal processing and deep learning stages. The Mamba Classifier outperformed other models, achieving an accuracy of 95.14% and an AUC of 0.9922 in the 25% hold-out validation. Transformer and Stacking models also demonstrated strong performance, while the Voting and XGBoost models showed comparatively lower results. When combined with artificial intelligence techniques, the findings indicate the potential of BCG signals in providing non-invasive, long-term hypertension detection. The results suggest that the Mamba Classifier is the most effective model for this dataset. This research underscores the potential of BCG technology for continuous home-based health monitoring, providing a feasible alternative to traditional methods. Future research should aim to validate these findings with larger datasets and explore the clinical applications of BCG for cardiovascular disease monitoring.\", 'publication_date': datetime.date(2025, 3, 10)}, {'title': 'Predicting amyloid proteins using attention-based long short-term memory.', 'authors': ['Li, Zhuowen'], 'abstract': \"Alzheimer's disease (AD) is one of the genetically inherited neurodegenerative disorders that mostly occur when people get old. It can be recognized by severe memory impairment in the late stage, affecting cognitive function and general daily living. Reliable evidence confirms that the enhanced symptoms of AD are linked to the accumulation of amyloid proteins. The dense population of amyloid proteins forms insoluble fibrillar structures, causing significant pathological impacts in various tissues. Understanding amyloid protein's mechanisms and identifying them at an early stage plays an essential role in treating AD as well as prevalent amyloid-related diseases. Recently, although several machine learning methods proposed for amyloid protein identification have shown promising results, most of them have not yet fully exploited the sequence information of the amyloid proteins. In this study, we develop a computational model for \", 'publication_date': datetime.date(2025, 3, 10)}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "def convert_pubmed_resp_to_str(articles):\n",
    "    master_str = \"\"\n",
    "    for article in articles:\n",
    "        formatted_string = f\"\"\"Title: {article['title']}\n",
    "        Authors: {', '.join(article['authors'])}\n",
    "        Abstract: {article['abstract']}\n",
    "        Publication Date: {article['publication_date'].strftime('%Y-%m-%d')}\n",
    "        {'-' * 80}\\n\"\"\"\n",
    "        master_str += formatted_string\n",
    "    return str(master_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = convert_pubmed_resp_to_str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(completion.choices[0].message)  # append model's function call message\n",
    "messages.append({                               # append result message\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": results_str\n",
    "})\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here is a summary of recent research articles related to transformers in machine learning:\\n\\n1. **Accelerating the Inference of String Generation-Based Chemical Reaction Models for Industrial Applications**\\n   - **Authors**: Andronov, Mikhail; Andronova, Natalia; Wand, Michael; Schmidhuber, Jürgen; Clevert, Djork-Arné\\n   - **Abstract**: This study focuses on improving the inference speed of transformer-based SMILES-to-SMILES translation models used for reaction prediction and retrosynthesis. The authors introduce speculative decoding and Speculative Beam Search to achieve over 3 times faster inference without sacrificing accuracy. This advancement enhances the practical utility of transformers in synthesis planning systems.\\n   - **Publication Date**: March 11, 2025\\n\\n2. **Principal Component Analysis and Fine-Tuned Vision Transformation Integrating Model Explainability for Breast Cancer Prediction**\\n   - **Authors**: Luong, Huong Hoang; Hong, Phuc Phan; Minh, Dat Vo; Quang, Thinh Nguyen Le; The, Anh Dinh; Thai-Nghe, Nguyen; Nguyen, Hai Thanh\\n   - **Abstract**: The study employs Vision Transformers (ViT) to classify breast cancer tumors. Enhancements include the use of multi-head locality large kernel self-attention and principal component analysis for improved accuracy and reduced computational complexity. Interpretable machine learning techniques are used to understand feature importance, achieving a significant accuracy improvement.\\n   - **Publication Date**: March 10, 2025\\n\\n3. **A Novel Integrative Multimodal Classifier to Enhance the Diagnosis of Parkinson's Disease**\\n   - **Authors**: Zhou, Xiaoyan; Parisi, Luca; Huang, Wentao; Zhang, Yihan; Huang, Xiaoqun; Youseffi, Mansour; Javid, Farideh; Ma, Renfei\\n   - **Abstract**: This research explores a multimodal predictive model integrating diverse data (such as hematological information and imaging) to detect Parkinson's disease. A multimodal encoder-based model with multi-head cross-attention (MMT_CA) yielded high predictive accuracy. The study also highlights potential precision medicine applications.\\n   - **Publication Date**: March 10, 2025\\n\\n4. **Non-Invasive Enhanced Hypertension Detection Through Ballistocardiograph Signals with Mamba Model**\\n   - **Authors**: Alhudhaif, Adi; Polat, Kemal\\n   - **Abstract**: This research introduces a novel approach using ballistocardiography signals and transformer-based models for hypertension detection. The Mamba deep learning architecture is employed for effective long-term signal analysis. The study suggests this method for potential continuous home-based health monitoring.\\n   - **Publication Date**: March 10, 2025\\n\\n5. **Predicting Amyloid Proteins Using Attention-Based Long Short-Term Memory**\\n   - **Authors**: Li, Zhuowen\\n   - **Abstract**: This study develops a model for the early identification of amyloid proteins, critical for addressing Alzheimer's disease. While the full abstract was not available, the use of attention-based long short-term memory suggests innovative exploitation of sequence information for amyloid protein identification.\\n   - **Publication Date**: March 10, 2025\\n\\nThese articles highlight various applications and advancements of transformer models in diverse fields such as chemistry, healthcare, and neurodegenerative disease diagnostics. They offer a range of research ideas for applying transformer models in biological and medical data processing, as well as industrial applications.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))\n"
     ]
    }
   ],
   "source": [
    "print(completion_2.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagram saved as diagram1.svg\n",
      "Diagram saved as diagram2.png\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def save_mermaid_concept_map_as_image(mermaid_code: str, output_file: str = \"output.svg\"):\n",
    "    \"\"\"\n",
    "    Saves a Mermaid diagram as an image using the Mermaid.ink API.\n",
    "    \n",
    "    Args:\n",
    "        mermaid_code (str): The Mermaid code as a string.\n",
    "        output_file (str): The output file name (supports .png, .svg, .pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    if not output_file.endswith((\".png\", \".svg\", \".pdf\")):\n",
    "        raise ValueError(\"Output file must have a valid extension (.png, .svg, .pdf)\")\n",
    "    elif not mermaid_code:\n",
    "        raise ValueError(\"Mermaid code cannot be empty\")\n",
    "    \n",
    "    if output_file.endswith(\".png\"):\n",
    "        mermaid_api_url = \"https://mermaid.ink/img/\"\n",
    "    elif output_file.endswith(\".svg\"):\n",
    "        mermaid_api_url = \"https://mermaid.ink/svg/\"\n",
    "    \n",
    "    # Encode Mermaid code to Base64\n",
    "    encoded_diagram = base64.urlsafe_b64encode(mermaid_code.encode()).decode()\n",
    "    \n",
    "    # Generate the full API URL\n",
    "    image_url = f\"{mermaid_api_url}{encoded_diagram}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the image from the API\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save the image\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Diagram saved as {output_file}\")\n",
    "\n",
    "        return output_file\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "\n",
    "# Example usage\n",
    "mermaid_code = \"\"\"\n",
    "  graph TD;\n",
    "    A-->B;\n",
    "    A-->C;\n",
    "    B-->D;\n",
    "    C-->D;\n",
    "\"\"\"\n",
    "save_mermaid_concept_map_as_image(mermaid_code, \"diagram1.svg\")\n",
    "save_mermaid_concept_map_as_image(mermaid_code, \"diagram2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"save_mermaid_as_image\",\n",
    "        \"description\": \"Saves a Mermaid diagram as an image using the Mermaid.ink API.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"mermaid_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The Mermaid diagram code as a string.\"\n",
    "                },\n",
    "                \"output_file\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The output file name (supports .png, .svg, .pdf). Defaults to 'output.svg'.\",\n",
    "                    \"default\": \"output.svg\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"mermaid_code\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vas = \"\"\"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \n",
    "You ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \n",
    "\n",
    "<First Step>\n",
    "Your task is to lightly interview the user;\n",
    "\n",
    "Start with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \n",
    "Ask one question per turn. \n",
    "Don't ask more than 5 questions in total.\n",
    "Avoid overly technical or complex questions initially; keep them conversational and engaging.\n",
    "Let the conversation flow naturally to build trust and comfort.\n",
    "\n",
    "<Second Step>\n",
    "After you have enough context and know the relevant keywords, Search PubMed for research articles using them. \n",
    "\n",
    "<Third Step>\n",
    "After you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\n",
    "Also add the names of the articles as sources at the end of the response.\n",
    "\n",
    "<Forth Step>\n",
    "Generate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics. Use the function save_mermaid_concept_map_as_image to save the mermaid diagram and return the output path.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics. Use the function save_mermaid_concept_map_as_image to save the mermaid diagram and return the output path.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoFile Link: https://gofile.io/d/Cee8B2\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def upload_to_gofile(image_path):\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        response = requests.post(\"https://store1.gofile.io/uploadFile\", files={\"file\": file})\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"data\"][\"downloadPage\"]\n",
    "\n",
    "# Example usage\n",
    "gofile_link = upload_to_gofile(\"image.png\")\n",
    "print(\"GoFile Link:\", gofile_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     40\u001b[0m mermaid_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m  graph TD;\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m    A-->B;\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m    C-->D;\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 47\u001b[0m file_link \u001b[38;5;241m=\u001b[39m \u001b[43msave_mermaid_as_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmermaid_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Link:\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_link)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36msave_mermaid_as_image\u001b[0;34m(mermaid_code)\u001b[0m\n\u001b[1;32m     20\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_api_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_diagram\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Fetch the image from the API\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Upload directly to GoFile without saving locally\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def save_mermaid_as_image(mermaid_code: str):\n",
    "    \"\"\"\n",
    "    Generates a Mermaid diagram as an image using the Mermaid.ink API and uploads it to GoFile.\n",
    "    \n",
    "    Args:\n",
    "        mermaid_code (str): The Mermaid code as a string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Shareable GoFile link.\n",
    "    \"\"\"\n",
    "    mermaid_api_url = \"https://mermaid.ink/img/\"\n",
    "    \n",
    "    # Encode Mermaid code to Base64\n",
    "    encoded_diagram = base64.urlsafe_b64encode(mermaid_code.encode()).decode()\n",
    "    \n",
    "    # Generate the full API URL\n",
    "    image_url = f\"{mermaid_api_url}{encoded_diagram}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the image from the API\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Upload directly to GoFile without saving locally\n",
    "        files = {\"file\": (\"diagram.png\", response.content, \"image/png\")}\n",
    "        upload_response = requests.post(\"https://store1.gofile.io/uploadFile\", files=files)\n",
    "        upload_response.raise_for_status()\n",
    "        \n",
    "        gofile_link = upload_response.json()[\"data\"][\"downloadPage\"]\n",
    "        print(f\"Shareable Link: {gofile_link}\")\n",
    "        return gofile_link\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error generating or uploading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "mermaid_code = \"\"\"\n",
    "  graph TD;\n",
    "    A-->B;\n",
    "    A-->C;\n",
    "    B-->D;\n",
    "    C-->D;\n",
    "\"\"\"\n",
    "file_link = save_mermaid_as_image(mermaid_code)\n",
    "print(\"Final Link:\", file_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
