{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \n",
    "You ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \n",
    "\n",
    "<First Step>\n",
    "Your task is to lightly interview the user;\n",
    "\n",
    "Start with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \n",
    "Ask one question per turn. \n",
    "Don't ask more than 5 questions in total.\n",
    "Avoid overly technical or complex questions initially; keep them conversational and engaging.\n",
    "Let the conversation flow naturally to build trust and comfort.\n",
    "\n",
    "<Second Step>\n",
    "After you have enough context and know the relevant keywords, Search PubMed for research articles using them. \n",
    "\n",
    "<Third Step>\n",
    "After you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\n",
    "Also add the names of the articles as sources at the end of the response.\n",
    "\n",
    "<Forth Step>\n",
    "Generate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pymed import PubMed\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Check your .env file.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def pubmed_search(query, max_results=5):\n",
    "    pubmed = PubMed(tool=\"MyTool\", email=\"sharozhaseeb1@gmail.com\")\n",
    "    results = pubmed.query(query, max_results=max_results)\n",
    "\n",
    "    result_list = []\n",
    "    for article in results:\n",
    "        child_dict = {}\n",
    "        child_dict['title'] = article.title\n",
    "        child_dict['authors'] = [((author['lastname'] or '') + ', ' + (author['firstname'] or '')).strip(', ') for author in article.authors]\n",
    "        child_dict['abstract'] = article.abstract\n",
    "        child_dict['publication_date'] = article.publication_date\n",
    "        result_list.append(child_dict)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"pubmed_search\",\n",
    "        \"description\": \"Searches PubMed for research articles based on a given query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The keyword to search for.\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The maximum number of results to retrieve.\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Concept Map(Mermaid Diagram) that visualizes various research directions for the current context. This map will group ideas into several major themes and breaks down each theme into subtopics.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I work in machine learning specifically transformers, can you get me some research ideas?\"}]\n",
    "            # {\"role\": \"user\", \"content\": \"Hi, who are you ?\"}]\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    # tool_choice=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_euOxD1SrpSINfZ6O8fzSWs5J', function=Function(arguments='{\"query\":\"transformers machine learning\",\"max_results\":5}', name='pubmed_search'), type='function')], annotations=[]))\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool_calls'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].finish_reason\n",
    "\n",
    "#output for tool call will be 'tool_calls'\n",
    "#output for chat will be 'stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_euOxD1SrpSINfZ6O8fzSWs5J', 'function': {'arguments': '{\"query\":\"transformers machine learning\",\"max_results\":5}', 'name': 'pubmed_search'}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "tool_calls = completion.choices[0].message.tool_calls\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    print(tool_call.to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = completion.choices[0].message.tool_calls[0]\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "result = pubmed_search(args[\"query\"], args[\"max_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Searching PubMed for articles...\n",
      "Query: creatine supplementation\n",
      "Max Results: 5\n",
      "--------------------------------------------\n",
      "[{'title': '', 'authors': ['Guo, Minghan', 'Zhao, Lina', 'Cao, Li', 'Li, Xuan', 'Zhang, Jie', 'Dong, Yao', 'Wu, Ying', 'Gu, Shaobin'], 'abstract': 'Adequate protein consumption is essential for optimal physical fitness and enhancing athletic performance. This study explored the impact of ', 'publication_date': datetime.date(2025, 3, 13)}, {'title': 'Are there Effective Vegan-Friendly Supplements for Optimizing Health and Sports Performance? a Narrative Review.', 'authors': ['Nieto, Álvaro Vergara A', 'Diaz, Andrés Halabi', 'Hernández, Millaray'], 'abstract': 'Veganism, characterized by the exclusion of all animal-derived products, has grown in popularity due to ethical, environmental, and health considerations. However, vegan athletes often face unique nutritional challenges related to dietary deficiencies of critical nutrients such as proteins, vitamin B12, iron, calcium, and omega-3 fatty acids, among others. This narrative review aims to explore the efficacy and benefits of vegan-friendly supplements specifically tailored to athletic performance, focusing on essential micronutrients, ergogenic aids, and nutrient bioavailability.\\nNineteen key supplements are discussed, including protein powders, creatine, beta-alanine, caffeine, vitamin B12, vitamin D, omega-3 fatty acids, zinc, calcium, iron, iodine, vitamin K2, selenium, probiotics, nitrates, electrolytes (including sodium and potassium), taurine, vitamin A, and magnesium. Evidence suggests that the integration of these supplements into personalized nutrition plans can bridge dietary gaps while addressing specific performance needs, potentially leveling the competitive field for vegan athletes. Recent studies also highlight research gaps in sex-specific needs, synergistic effects, and strategies to enhance the bioavailability of nutrients from whole foods. Vegan diets, while conferring various benefits, require careful consideration of nutrient intake for athletes seeking optimal performance. Personalized biochemical assessments should be considered when possible for tailoring specific nutritional guidelines for each case. This narrative review provides practical guidelines for clinicians, nutritionists, trainers, sports scientists, and athletes to design personalized supplementation strategies that address common nutritional shortfalls, enhance performance, and serve as a foundation for future research in vegan sports nutrition.', 'publication_date': datetime.date(2025, 3, 12)}, {'title': 'Study on the Effect of Bifidobacterium adolescentis CCFM1066 on Exercise Performance, Gut Microbiota, and Its Metabolites in Mice.', 'authors': ['Wang, Hongchao', 'Ma, Huizi', 'Yan, Huimin', 'Pei, Zhangming', 'Zhao, Jianxin', 'Zhang, Hao', 'Zhang, Zhijian', 'Lu, Wenwei'], 'abstract': 'Prolonged high-intensity exercise consumes significant energy, leading to fatigue and decreased performance. This study explores the effects of Bifidobacterium adolescentis CCFM1066 on exercise performance, gut microbiota, and its metabolites in mice. The results of the mouse experiments showed the mice which were intervened by Bifidobacterium adolescentis CCFM1066 have a significant increase in exercise performance, including forceful swimming time, fatigue baton turning time, and forelimb grip strength. Through metagenomic sequencing and differential metabolites, analysis indicated that the intervention of CCFM1066 increased Lachnospiraceae bacterium, Parabacteroides goldsteinii, Bacteroides xylanisolvens, and Bifidobacterium adolescentis and altered the key metabolic pathways including protein digestion and absorption and biosynthesis of amino acids. Supplementation with CCFM1066 modulates the production of short-chain fatty acids (SCFAs) and fatty acid amides (FAAs) by gut microbiota, decreasing levels of lactic acid (LA), blood urea nitrogen (BUN), lactate dehydrogenase (LDH), and creatine kinase (CK) while increasing muscle and hepatic glycogen content, thus reducing central nervous system fatigue and thereby improving exercise endurance and performance. These findings provide new insights into nutritional interventions for sports performance.', 'publication_date': datetime.date(2025, 3, 11)}, {'title': 'Unraveling an Uncommon Encounter: Hypokalemic Periodic Paralysis with Brugada Phenocopy Amidst Hypokalemia.', 'authors': ['Dankar, Razan', 'Barakat, Salim', 'El-Charabaty, Elie', 'Hashmi, Syed Salman Hamid', 'El Sayegh, Suzanne E'], 'abstract': 'Hypokalemic periodic paralysis (HPP) presents a diagnostic challenge due to the painless muscle weakness it causes. This case discusses a patient with HPP along with electrocardiogram (ECG) findings of Brugada phenocopies (BrP) in the setting of hypokalemia. A review of the literature showed that it is the seventh documented example of BrP induced by hypokalemia alongside HPP.\\nA 43-year-old man presented to the emergency department with lower limb weakness. He attributed his symptoms to a substantial meal consumed after breaking his Ramadan fast, recalling a similar episode following heavy meals in the past. The patient was alert and oriented but demonstrated reduced strength in both upper and lower limbs. ECG revealed a Brugada type 1 pattern. Laboratory analysis revealed hypokalemia (2.5 mmol/l), elevated creatine kinase (326 U/l), and normal thyroid function. Following potassium supplementation, his symptoms resolved, and his ECG normalized.\\nHPP occurs in the context of increased carbohydrate intake, potentially leading to rapid insulin release and activation of Na-K ATPase, enhancing cellular potassium absorption and lowering serum potassium levels. Symptoms range from weakness and fatigue to severe neuromuscular weakness and cardiac arrhythmias. Investigating hypokalemia requires excluding hypomagnesemia, thyroid function tests, and metabolic acidosis/alkalosis before considering HPP. Management involves gradual oral potassium repletion to avoid the risk of hyperkalemia associated with intravenous administration.\\nClinicians should consider including HPP in differential diagnoses of patients presenting with weakness. In this case, electrophysiological evaluation suggested Brugada pattern induced by hypokalemia, which resolved with potassium supplementation.\\nThis case highlights the rarity and diagnostic challenges of hypokalemic periodic paralysis, offering critical insights into recognizing and managing such conditions in clinical practice.The case also demonstrates the importance of identifying reversible electrocardiogram changes like Brugada patterns, aiding in differentiation from persistent arrhythmias and avoiding unnecessary interventions.', 'publication_date': datetime.date(2025, 3, 7)}, {'title': 'D-BHB supplementation before moderate-intensity exercise suppresses lipolysis and selectively blunts exercise-induced long-chain acylcarnitine increase in pilot study of patients with long-chain fatty acid oxidation disorders.', 'authors': ['Gregor, Ashley N', 'Delerive, Philippe', 'Cuenoud, Bernard', 'Monnard, Irina', 'Redeuil, Karine', 'Harding, Cary O', 'Gillingham, Melanie B'], 'abstract': 'Patients with long-chain fatty acid oxidation disorders (LC-FAOD) have impaired endogenous ketone production due to defects in the beta-oxidation pathway. We explored supplementation of exogenous D-beta-hydroxybutyrate (D-BHB) as an alternative source of energy in a randomized, double-blinded crossover pilot study. Participants ≥18\\xa0years of age with a diagnosis of LC-FAOD completed two moderate-intensity treadmill exercises following an oral supplementation of D-BHB salts or an isocaloric maltodextrin beverage. Five subjects (1 VLCADD, 2 CPT2D, 2 LCHADD), 60\\xa0% male, mean age\\xa0=\\xa033\\xa0years were enrolled. Mild to moderate GI symptoms were related to ingestion of D-BHB. Plasma D-BHB was increased after oral D-BHB compared to maltodextrin (p\\xa0<\\xa0.001) with an average concentration of 0.43\\xa0mM in the post-exercise period. During exercise, free fatty acids (p\\xa0=\\xa0.01), fold change in long-chain acylcarnitine species (LC-AC) (p\\xa0≤\\xa0.03) and systolic BP (p\\xa0=\\xa0.02) were lower after D-BHB compared to the maltodextrin beverage. D-BHB suppresses lipolysis and selectively blunts exercise-induced long-chain acylcarnitines. There were no differences between beverages in acetylcarnitine, blood glucose, creatine kinase, VO2, HR, RPE or respiratory exchange ratio. Consumption of the D-BHB beverage was safe and well-tolerated. Plasma D-BHB levels achieved mild ketosis and suppressed lipolysis and the associated rise in LC-AC, but fell short of stimulating the energetic effects that might have resulted in altered exercise parameters such as RER, or HR. In conclusion, our results provide a strong rationale for future studies aimed toward defining the optimal multiple-dose regimen of D-BHB per day that might improve exercise tolerance and understanding the long-term impact of treatment in LC-FAOD subjects.', 'publication_date': datetime.date(2025, 3, 7)}]\n"
     ]
    }
   ],
   "source": [
    "# from helper import pubmed_search\n",
    "# print(pubmed_search(\"creatine supplementation\", max_results=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Accelerating the inference of string generation-based chemical reaction models for industrial applications.', 'authors': ['Andronov, Mikhail', 'Andronova, Natalia', 'Wand, Michael', 'Schmidhuber, Jürgen', 'Clevert, Djork-Arné'], 'abstract': \"Transformer-based, template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest to computer-aided synthesis planning systems, as they offer state-of-the-art accuracy. However, their slow inference speed limits their practical utility in such applications. To address this challenge, we propose speculative decoding with a simple chemically specific drafting strategy and apply it to the Molecular Transformer, an encoder-decoder transformer for conditional SMILES generation. Our approach achieves over 3X faster inference in reaction product prediction and single-step retrosynthesis with no loss in accuracy, increasing the potential of the transformer as the backbone of synthesis planning systems. To accelerate the simultaneous generation of multiple precursor SMILES for a given query SMILES in single-step retrosynthesis, we introduce Speculative Beam Search, a novel algorithm tackling the challenge of beam search acceleration with speculative decoding. Our methods aim to improve transformer-based models' scalability and industrial applicability in synthesis planning.\", 'publication_date': datetime.date(2025, 3, 11)}, {'title': 'Principal component analysis and fine-tuned vision transformation integrating model explainability for breast cancer prediction.', 'authors': ['Luong, Huong Hoang', 'Hong, Phuc Phan', 'Minh, Dat Vo', 'Quang, Thinh Nguyen Le', 'The, Anh Dinh', 'Thai-Nghe, Nguyen', 'Nguyen, Hai Thanh'], 'abstract': 'Breast cancer, which is the most commonly diagnosed cancers among women, is a notable health issues globally. Breast cancer is a result of abnormal cells in the breast tissue growing out of control. Histopathology, which refers to the detection and learning of tissue diseases, has appeared as a solution for breast cancer treatment as it plays a vital role in its diagnosis and classification. Thus, considerable research on histopathology in medical and computer science has been conducted to develop an effective method for breast cancer treatment. In this study, a vision Transformer (ViT) was employed to classify tumors into two classes, benign and malignant, in the Breast Cancer Histopathological Database (BreakHis). To enhance the model performance, we introduced the novel multi-head locality large kernel self-attention during fine-tuning, achieving an accuracy of 95.94% at 100× magnification, thereby improving the accuracy by 3.34% compared to a standard ViT (which uses multi-head self-attention). In addition, the application of principal component analysis for dimensionality reduction led to an accuracy improvement of 3.34%, highlighting its role in mitigating overfitting and reducing the computational complexity. In the final phase, SHapley Additive exPlanations, Local Interpretable Model-agnostic Explanations, and Gradient-weighted Class Activation Mapping were used for the interpretability and explainability of machine-learning models, aiding in understanding the feature importance and local explanations, and visualizing the model attention. In another experiment, ensemble learning with VGGIN further boosted the performance to 97.13% accuracy. Our approach exhibited a 0.98% to 17.13% improvement in accuracy compared with state-of-the-art methods, establishing a new benchmark for breast cancer histopathological image classification.', 'publication_date': datetime.date(2025, 3, 10)}, {'title': \"A novel integrative multimodal classifier to enhance the diagnosis of Parkinson's disease.\", 'authors': ['Zhou, Xiaoyan', 'Parisi, Luca', 'Huang, Wentao', 'Zhang, Yihan', 'Huang, Xiaoqun', 'Youseffi, Mansour', 'Javid, Farideh', 'Ma, Renfei'], 'abstract': \"Parkinson's disease (PD) is a complex, progressive neurodegenerative disorder with high heterogeneity, making early diagnosis difficult. Early detection and intervention are crucial for slowing PD progression. Understanding PD's diverse pathways and mechanisms is key to advancing knowledge. Recent advances in noninvasive imaging and multi-omics technologies have provided valuable insights into PD's underlying causes and biological processes. However, integrating these diverse data sources remains challenging, especially when deriving meaningful low-level features that can serve as diagnostic indicators. This study developed and validated a novel integrative, multimodal predictive model for detecting PD based on features derived from multimodal data, including hematological information, proteomics, RNA sequencing, metabolomics, and dopamine transporter scan imaging, sourced from the Parkinson's Progression Markers Initiative. Several model architectures were investigated and evaluated, including support vector machine, eXtreme Gradient Boosting, fully connected neural networks with concatenation and joint modeling (FCNN_C and FCNN_JM), and a multimodal encoder-based model with multi-head cross-attention (MMT_CA). The MMT_CA model demonstrated superior predictive performance, achieving a balanced classification accuracy of 97.7%, thus highlighting its ability to capture and leverage cross-modality inter-dependencies to aid predictive analytics. Furthermore, feature importance analysis using SHapley Additive exPlanations not only identified crucial diagnostic biomarkers to inform the predictive models in this study but also holds potential for future research aimed at integrated functional analyses of PD from a multi-omics perspective, ultimately revealing targets required for precision medicine approaches to aid treatment of PD aimed at slowing down its progression.\", 'publication_date': datetime.date(2025, 3, 10)}, {'title': 'Non-invasive enhanced hypertension detection through ballistocardiograph signals with Mamba model.', 'authors': ['Alhudhaif, Adi', 'Polat, Kemal'], 'abstract': \"This study explores using ballistocardiography (BCG), a non-invasive cardiovascular monitoring technique, combined with advanced machine learning and deep learning models for hypertension detection. The motivation behind this research is to develop a non-invasive and efficient approach for long-term hypertension monitoring, facilitating home-based health assessments. A dataset of 128 BCG recordings has been used, capturing body micro-vibrations from cardiac activity. Various classification models, including Mamba Classifier, Transformer, Stacking, Voting, and XGBoost, were applied to differentiate hypertensive individuals from normotensive ones. In this study, integrating BCG signals with deep learning and machine learning models for hypertension detection is distinguished from previous literature by employing the Mamba deep learning architecture and Transformer-based models. Unlike conventional methods in literature, this study enables more effective analysis of time-series data with the Mamba architecture, capturing long-term signal dependencies and achieving higher accuracy rates. In particular, the combined use of Mamba architecture and the Transformer model's signal processing capabilities represents a novel approach not previously seen in the literature. While existing studies on BCG signals typically rely on traditional machine learning algorithms, this study aims to achieve higher success rates in hypertension detection by integrating signal processing and deep learning stages. The Mamba Classifier outperformed other models, achieving an accuracy of 95.14% and an AUC of 0.9922 in the 25% hold-out validation. Transformer and Stacking models also demonstrated strong performance, while the Voting and XGBoost models showed comparatively lower results. When combined with artificial intelligence techniques, the findings indicate the potential of BCG signals in providing non-invasive, long-term hypertension detection. The results suggest that the Mamba Classifier is the most effective model for this dataset. This research underscores the potential of BCG technology for continuous home-based health monitoring, providing a feasible alternative to traditional methods. Future research should aim to validate these findings with larger datasets and explore the clinical applications of BCG for cardiovascular disease monitoring.\", 'publication_date': datetime.date(2025, 3, 10)}, {'title': 'Predicting amyloid proteins using attention-based long short-term memory.', 'authors': ['Li, Zhuowen'], 'abstract': \"Alzheimer's disease (AD) is one of the genetically inherited neurodegenerative disorders that mostly occur when people get old. It can be recognized by severe memory impairment in the late stage, affecting cognitive function and general daily living. Reliable evidence confirms that the enhanced symptoms of AD are linked to the accumulation of amyloid proteins. The dense population of amyloid proteins forms insoluble fibrillar structures, causing significant pathological impacts in various tissues. Understanding amyloid protein's mechanisms and identifying them at an early stage plays an essential role in treating AD as well as prevalent amyloid-related diseases. Recently, although several machine learning methods proposed for amyloid protein identification have shown promising results, most of them have not yet fully exploited the sequence information of the amyloid proteins. In this study, we develop a computational model for \", 'publication_date': datetime.date(2025, 3, 10)}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "def convert_pubmed_resp_to_str(articles):\n",
    "    master_str = \"\"\n",
    "    for article in articles:\n",
    "        formatted_string = f\"\"\"Title: {article['title']}\n",
    "        Authors: {', '.join(article['authors'])}\n",
    "        Abstract: {article['abstract']}\n",
    "        Publication Date: {article['publication_date'].strftime('%Y-%m-%d')}\n",
    "        {'-' * 80}\\n\"\"\"\n",
    "        master_str += formatted_string\n",
    "    return str(master_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = convert_pubmed_resp_to_str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(completion.choices[0].message)  # append model's function call message\n",
    "messages.append({                               # append result message\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": results_str\n",
    "})\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here is a summary of recent research articles related to transformers in machine learning:\\n\\n1. **Accelerating the Inference of String Generation-Based Chemical Reaction Models for Industrial Applications**\\n   - **Authors**: Andronov, Mikhail; Andronova, Natalia; Wand, Michael; Schmidhuber, Jürgen; Clevert, Djork-Arné\\n   - **Abstract**: This study focuses on improving the inference speed of transformer-based SMILES-to-SMILES translation models used for reaction prediction and retrosynthesis. The authors introduce speculative decoding and Speculative Beam Search to achieve over 3 times faster inference without sacrificing accuracy. This advancement enhances the practical utility of transformers in synthesis planning systems.\\n   - **Publication Date**: March 11, 2025\\n\\n2. **Principal Component Analysis and Fine-Tuned Vision Transformation Integrating Model Explainability for Breast Cancer Prediction**\\n   - **Authors**: Luong, Huong Hoang; Hong, Phuc Phan; Minh, Dat Vo; Quang, Thinh Nguyen Le; The, Anh Dinh; Thai-Nghe, Nguyen; Nguyen, Hai Thanh\\n   - **Abstract**: The study employs Vision Transformers (ViT) to classify breast cancer tumors. Enhancements include the use of multi-head locality large kernel self-attention and principal component analysis for improved accuracy and reduced computational complexity. Interpretable machine learning techniques are used to understand feature importance, achieving a significant accuracy improvement.\\n   - **Publication Date**: March 10, 2025\\n\\n3. **A Novel Integrative Multimodal Classifier to Enhance the Diagnosis of Parkinson's Disease**\\n   - **Authors**: Zhou, Xiaoyan; Parisi, Luca; Huang, Wentao; Zhang, Yihan; Huang, Xiaoqun; Youseffi, Mansour; Javid, Farideh; Ma, Renfei\\n   - **Abstract**: This research explores a multimodal predictive model integrating diverse data (such as hematological information and imaging) to detect Parkinson's disease. A multimodal encoder-based model with multi-head cross-attention (MMT_CA) yielded high predictive accuracy. The study also highlights potential precision medicine applications.\\n   - **Publication Date**: March 10, 2025\\n\\n4. **Non-Invasive Enhanced Hypertension Detection Through Ballistocardiograph Signals with Mamba Model**\\n   - **Authors**: Alhudhaif, Adi; Polat, Kemal\\n   - **Abstract**: This research introduces a novel approach using ballistocardiography signals and transformer-based models for hypertension detection. The Mamba deep learning architecture is employed for effective long-term signal analysis. The study suggests this method for potential continuous home-based health monitoring.\\n   - **Publication Date**: March 10, 2025\\n\\n5. **Predicting Amyloid Proteins Using Attention-Based Long Short-Term Memory**\\n   - **Authors**: Li, Zhuowen\\n   - **Abstract**: This study develops a model for the early identification of amyloid proteins, critical for addressing Alzheimer's disease. While the full abstract was not available, the use of attention-based long short-term memory suggests innovative exploitation of sequence information for amyloid protein identification.\\n   - **Publication Date**: March 10, 2025\\n\\nThese articles highlight various applications and advancements of transformer models in diverse fields such as chemistry, healthcare, and neurodegenerative disease diagnostics. They offer a range of research ideas for applying transformer models in biological and medical data processing, as well as industrial applications.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))\n"
     ]
    }
   ],
   "source": [
    "print(completion_2.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "pako:CmdyYXBoIFREOwogICAgQVtTdGFydF0gLS0-fFN0ZXAgMXwgQltQcm9jZXNzIDFdOwogICAgQiAtLT4gfERlY2lzaW9ufCBDe0lzIGl0IHZhbGlkP307CiAgICBDIC0tIFllcyAtLT4gRFtDb250aW51ZSBQcm9jZXNzXTsKICAgIEMgLS0gTm8gLS0-IEVbRml4IElzc3VlXTsKICAgIEQgLS0-IEZbRW5kXTsK\n",
      "here2\n",
      "https://mermaid.ink/img/pako:CmdyYXBoIFREOwogICAgQVtTdGFydF0gLS0-fFN0ZXAgMXwgQltQcm9jZXNzIDFdOwogICAgQiAtLT4gfERlY2lzaW9ufCBDe0lzIGl0IHZhbGlkP307CiAgICBDIC0tIFllcyAtLT4gRFtDb250aW51ZSBQcm9jZXNzXTsKICAgIEMgLS0gTm8gLS0-IEVbRml4IElzc3VlXTsKICAgIEQgLS0-IEZbRW5kXTsK\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def save_mermaid_concept_map_as_image(mermaid_code: str, output_file: str = \"output.svg\"):\n",
    "    \"\"\"\n",
    "    Saves a Mermaid diagram as an image using the Mermaid.ink API.\n",
    "    \n",
    "    Args:\n",
    "        mermaid_code (str): The Mermaid code as a string.\n",
    "        output_file (str): The output file name (supports .png, .svg, .pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    if not output_file.endswith((\".png\", \".svg\", \".pdf\")):\n",
    "        raise ValueError(\"Output file must have a valid extension (.png, .svg, .pdf)\")\n",
    "    elif not mermaid_code:\n",
    "        raise ValueError(\"Mermaid code cannot be empty\")\n",
    "    \n",
    "    if output_file.endswith(\".png\"):\n",
    "        mermaid_api_url = \"https://mermaid.ink/img/\"\n",
    "    elif output_file.endswith(\".svg\"):\n",
    "        mermaid_api_url = \"https://mermaid.ink/svg/\"\n",
    "    print(\"here\")\n",
    "    # Encode Mermaid code to Base64\n",
    "    encoded_diagram = f\"pako:{base64.urlsafe_b64encode(mermaid_code.encode()).decode()}\"\n",
    "\n",
    "    print(encoded_diagram)\n",
    "    print(\"here2\")\n",
    "\n",
    "    # # Generate the full API URL\n",
    "    image_url = f\"{mermaid_api_url}{encoded_diagram}\"\n",
    "    print(image_url)\n",
    "    # try:\n",
    "    #     # Fetch the image from the API\n",
    "    #     response = requests.get(image_url)\n",
    "    #     print(\"here3: \", response)\n",
    "    #     response.raise_for_status()\n",
    "        \n",
    "    #     # Save the image\n",
    "    #     with open(output_file, \"wb\") as file:\n",
    "    #         file.write(response.content)\n",
    "    #     print(f\"Diagram saved as {output_file}\")\n",
    "\n",
    "    #     return output_file\n",
    "    # except requests.RequestException as e:\n",
    "    #     print(f\"Error generating image: {e}\")\n",
    "\n",
    "# Example usage\n",
    "mermaid_code = \"\"\"\n",
    "graph TD;\n",
    "    A[Start] -->|Step 1| B[Process 1];\n",
    "    B --> |Decision| C{Is it valid?};\n",
    "    C -- Yes --> D[Continue Process];\n",
    "    C -- No --> E[Fix Issue];\n",
    "    D --> F[End];\n",
    "\"\"\"\n",
    "# save_mermaid_concept_map_as_image(mermaid_code, \"diagram1.svg\")\n",
    "save_mermaid_concept_map_as_image(mermaid_code, \"diagram2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mermaid-py\n",
      "  Downloading mermaid_py-0.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from mermaid-py) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->mermaid-py) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->mermaid-py) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->mermaid-py) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->mermaid-py) (2024.2.2)\n",
      "Downloading mermaid_py-0.7.1-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: mermaid-py\n",
      "Successfully installed mermaid-py-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mermaid-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"save_mermaid_as_image\",\n",
    "        \"description\": \"Saves a Mermaid diagram as an image using the Mermaid.ink API.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"mermaid_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The Mermaid diagram code as a string.\"\n",
    "                },\n",
    "                \"output_file\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The output file name (supports .png, .svg, .pdf). Defaults to 'output.svg'.\",\n",
    "                    \"default\": \"output.svg\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"mermaid_code\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vas = \"\"\"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \n",
    "You ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \n",
    "\n",
    "<First Step>\n",
    "Your task is to lightly interview the user;\n",
    "\n",
    "Start with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \n",
    "Ask one question per turn. \n",
    "Don't ask more than 5 questions in total.\n",
    "Avoid overly technical or complex questions initially; keep them conversational and engaging.\n",
    "Let the conversation flow naturally to build trust and comfort.\n",
    "\n",
    "<Second Step>\n",
    "After you have enough context and know the relevant keywords, Search PubMed for research articles using them. \n",
    "\n",
    "<Third Step>\n",
    "After you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\n",
    "Also add the names of the articles as sources at the end of the response.\n",
    "\n",
    "<Forth Step>\n",
    "Generate a Mind Map(Mermaid Diagram) that visualizes various research directions for the current context. Major ideas are connected directly to the central concept, and other ideas branch out from those major ideas.\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Main Topic))\n",
    "    subtopic1[First Branch]\n",
    "      sub1a[Subtopic A]\n",
    "      sub1b[Subtopic B]\n",
    "    subtopic2[Second Branch]\n",
    "      sub2a[Another Idea]\n",
    "      sub2b[More Thoughts]\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a Professional Research Advisor who work for Research Boost. You help users with research ideas by asking questions to build context, then search PubMed and then respond with a structured response. \\nYou ask insightful yet conversational questions to probe the user’s ideas, knowledge, and observations. Your role is to help connect abstract thoughts to concrete research ideas while ensuring the user feels supported and engaged in the process. \\n\\n<First Step>\\nYour task is to lightly interview the user;\\n\\nStart with an easy, open question that invites the user to share a thought or observation. Build on their response with follow-up questions. \\nAsk one question per turn. \\nDon't ask more than 5 questions in total.\\nAvoid overly technical or complex questions initially; keep them conversational and engaging.\\nLet the conversation flow naturally to build trust and comfort.\\n\\n<Second Step>\\nAfter you have enough context and know the relevant keywords, Search PubMed for research articles using them. \\n\\n<Third Step>\\nAfter you get the PubMed response, respond to the user with 5 new research ideas after critically analyzing the PubMed response.\\nAlso add the names of the articles as sources at the end of the response.\\n\\n<Forth Step>\\nGenerate a Mind Map(Mermaid Diagram) that visualizes various research directions for the current context. Major ideas are connected directly to the central concept, and other ideas branch out from those major ideas.\\n\\n```mermaid\\nmindmap\\n  root((Main Topic))\\n    subtopic1[First Branch]\\n      sub1a[Subtopic A]\\n      sub1b[Subtopic B]\\n    subtopic2[Second Branch]\\n      sub2a[Another Idea]\\n      sub2b[More Thoughts]\\n```\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'createTime': 1742075828, 'downloadPage': 'https://gofile.io/d/cNWsqi', 'guestToken': 'qnFgJZpqWmg559qYTWPlSh8d4AdvpMkg', 'id': 'a9922c21-47d7-4f5a-b4aa-7e0106392be8', 'md5': 'd084f9988d4a063eec8c1bd726d8bd7e', 'mimetype': 'image/png', 'modTime': 1742075828, 'name': 'image.png', 'parentFolder': 'e1021904-f4d0-49ca-b089-ebbf016da11b', 'parentFolderCode': 'cNWsqi', 'servers': ['store1'], 'size': 16510, 'type': 'file'}, 'status': 'ok'}\n",
      "GoFile Link: https://gofile.io/d/cNWsqi\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def upload_to_gofile(image_path):\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        response = requests.post(\"https://store1.gofile.io/uploadFile\", files={\"file\": file})\n",
    "        response.raise_for_status()\n",
    "        print(response.json())\n",
    "        return response.json()[\"data\"][\"downloadPage\"]\n",
    "\n",
    "# Example usage\n",
    "gofile_link = upload_to_gofile(\"image.png\")\n",
    "print(\"GoFile Link:\", gofile_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Download Link: https://store1.gofile.io/download/web/9cf8b598-3f4c-434b-8065-234463542c8d/image.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def upload_to_gofile(image_path):\n",
    "    with open(image_path, \"rb\") as file:\n",
    "        response = requests.post(\"https://store1.gofile.io/uploadFile\", files={\"file\": file})\n",
    "        response.raise_for_status()\n",
    "        data = response.json()[\"data\"]\n",
    "        \n",
    "        # Construct the direct download link\n",
    "        server = data[\"servers\"][0]  # Get the first server\n",
    "        file_id = data[\"id\"]\n",
    "        file_name = data[\"name\"]\n",
    "        direct_download_link = f\"https://{server}.gofile.io/download/web/{file_id}/{file_name}\"\n",
    "        \n",
    "        return direct_download_link\n",
    "\n",
    "# Example usage\n",
    "gofile_link = upload_to_gofile(\"image.png\")\n",
    "print(\"Direct Download Link:\", gofile_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.1 MB 435.7 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.0/8.1 MB 435.7 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.1/8.1 MB 262.6 kB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.1/8.1 MB 403.5 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/8.1 MB 403.5 kB/s eta 0:00:20\n",
      "    --------------------------------------- 0.1/8.1 MB 379.3 kB/s eta 0:00:21\n",
      "    --------------------------------------- 0.2/8.1 MB 535.8 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.2/8.1 MB 599.0 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.2/8.1 MB 599.0 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.4/8.1 MB 740.5 kB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.5/8.1 MB 906.4 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/8.1 MB 906.4 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/8.1 MB 835.6 kB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.6/8.1 MB 914.3 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.8/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.0/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.2/8.1 MB 1.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.3/8.1 MB 1.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.5/8.1 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.6/8.1 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/8.1 MB 1.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/8.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.2/8.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.6/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.1 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.1 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/8.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.4/8.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.9/8.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/8.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/8.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.7 MB 3.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.6/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/221.0 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 112.6/221.0 kB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 215.0/221.0 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 221.0/221.0 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/107.7 kB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 107.7/107.7 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pyparsing, networkx, kiwisolver, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 kiwisolver-1.4.8 matplotlib-3.10.1 networkx-3.4.2 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Link: https://store1.gofile.io/download/web/c260d80a-0e58-4888-b64b-9cb05feb9c1e/mindmap.svg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def generate_and_upload_mindmap(mermaid_code):\n",
    "    \"\"\"\n",
    "    Parses Mermaid mindmap code, generates a mind map, and uploads it as an SVG to GoFile.\n",
    "    \n",
    "    Args:\n",
    "        mermaid_code (str): Mermaid mindmap syntax.\n",
    "    \n",
    "    Returns:\n",
    "        str: GoFile download link for the generated mind map SVG.\n",
    "    \"\"\"\n",
    "    def parse_mermaid_mindmap(code):\n",
    "        \"\"\"Converts Mermaid mindmap syntax into a list of edges for graph plotting.\"\"\"\n",
    "        lines = code.strip().split(\"\\n\")\n",
    "        edges = []\n",
    "        parent_stack = []\n",
    "\n",
    "        for line in lines:\n",
    "            indent_level = len(line) - len(line.lstrip())  # Count leading spaces\n",
    "            node = line.strip()\n",
    "\n",
    "            if node:\n",
    "                while parent_stack and parent_stack[-1][1] >= indent_level:\n",
    "                    parent_stack.pop()\n",
    "\n",
    "                if parent_stack:\n",
    "                    edges.append((parent_stack[-1][0], node))\n",
    "\n",
    "                parent_stack.append((node, indent_level))\n",
    "\n",
    "        return edges\n",
    "\n",
    "    def hierarchy_pos(G, root=None, width=1., vert_gap=0.4, xcenter=0.5, pos=None, level=0):\n",
    "        \"\"\"Assigns hierarchical positions for nodes in the tree.\"\"\"\n",
    "        if pos is None:\n",
    "            pos = {root: (xcenter, 1)}\n",
    "        children = list(G.successors(root))\n",
    "        if not children:\n",
    "            return pos\n",
    "\n",
    "        dx = width / max(1, len(children))\n",
    "        next_x = xcenter - width / 2 - dx / 2\n",
    "        for child in children:\n",
    "            next_x += dx\n",
    "            pos[child] = (next_x, 1 - vert_gap * (level + 1))\n",
    "            pos = hierarchy_pos(G, root=child, width=dx, vert_gap=vert_gap, xcenter=next_x, pos=pos, level=level+1)\n",
    "        return pos\n",
    "\n",
    "    # Step 1: Parse Mermaid mind map structure\n",
    "    edges = parse_mermaid_mindmap(mermaid_code)\n",
    "    if not edges:\n",
    "        raise ValueError(\"Invalid Mermaid mindmap format.\")\n",
    "\n",
    "    # Step 2: Create a graph using NetworkX\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    root = edges[0][0]  # Take the first node as root\n",
    "    pos = hierarchy_pos(G, root)\n",
    "\n",
    "    # Step 3: Generate the mind map image in SVG format\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\",\n",
    "            node_size=3000, font_size=4.5, font_weight=\"bold\", alpha=0.8, arrows=False)\n",
    "\n",
    "    svg_buffer = io.BytesIO()\n",
    "    plt.savefig(svg_buffer, format=\"svg\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    svg_buffer.seek(0)\n",
    "\n",
    "    # Step 4: Upload the SVG to GoFile\n",
    "    files = {\"file\": (\"mindmap.svg\", svg_buffer, \"image/svg+xml\")}\n",
    "    response = requests.post(\"https://store1.gofile.io/uploadFile\", files=files)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()[\"data\"]\n",
    "\n",
    "    # Construct the direct download link\n",
    "    server = data[\"servers\"][0]  # Use the first available server\n",
    "    file_id = data[\"id\"]\n",
    "    file_name = data[\"name\"]\n",
    "    return f\"https://{server}.gofile.io/download/web/{file_id}/{file_name}\"\n",
    "\n",
    "# Example Usage\n",
    "mermaid_code = \"\"\"\n",
    "mindmap\n",
    "  root((Short-term Effects of L-Carnitine for Weight Loss))\n",
    "    Lipid Metabolism\n",
    "      Enzyme Interaction\n",
    "    Energy Expenditure\n",
    "      Postprandial Effects\n",
    "      Basal Metabolism\n",
    "    Plasma Levels\n",
    "      Dietary Modifications\n",
    "      Urinary Concentrations\n",
    "    Gut Microbiota\n",
    "      Metabolomic Changes\n",
    "      Microbiota Composition\n",
    "    Thermogenic Effects\n",
    "      Thermogenesis\n",
    "      Pathway Inhibition\n",
    "\"\"\"\n",
    "download_link = generate_and_upload_mindmap(mermaid_code)\n",
    "print(\"Download Link:\", download_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Link: https://store1.gofile.io/download/web/e9010f0e-71d9-4821-91e9-985413cd246d/mindmap.png\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import io\n",
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "\n",
    "# def generate_and_upload_mindmap(mermaid_code):\n",
    "#     \"\"\"\n",
    "#     Parses Mermaid mindmap code, generates a mind map, and uploads it to GoFile.\n",
    "    \n",
    "#     Args:\n",
    "#         mermaid_code (str): Mermaid mindmap syntax.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: GoFile download link for the generated mind map image.\n",
    "#     \"\"\"\n",
    "#     def parse_mermaid_mindmap(code):\n",
    "#         \"\"\"Converts Mermaid mindmap syntax into a list of edges for graph plotting.\"\"\"\n",
    "#         lines = code.strip().split(\"\\n\")\n",
    "#         edges = []\n",
    "#         parent_stack = []\n",
    "\n",
    "#         for line in lines:\n",
    "#             indent_level = len(line) - len(line.lstrip())  # Count leading spaces\n",
    "#             node = line.strip()\n",
    "\n",
    "#             if node:\n",
    "#                 while parent_stack and parent_stack[-1][1] >= indent_level:\n",
    "#                     parent_stack.pop()\n",
    "\n",
    "#                 if parent_stack:\n",
    "#                     edges.append((parent_stack[-1][0], node))\n",
    "\n",
    "#                 parent_stack.append((node, indent_level))\n",
    "\n",
    "#         return edges\n",
    "\n",
    "#     def hierarchy_pos(G, root=None, width=1., vert_gap=0.4, xcenter=0.5, pos=None, level=0):\n",
    "#         \"\"\"Assigns hierarchical positions for nodes in the tree.\"\"\"\n",
    "#         if pos is None:\n",
    "#             pos = {root: (xcenter, 1)}\n",
    "#         children = list(G.successors(root))\n",
    "#         if not children:\n",
    "#             return pos\n",
    "\n",
    "#         dx = width / max(1, len(children))\n",
    "#         next_x = xcenter - width / 2 - dx / 2\n",
    "#         for child in children:\n",
    "#             next_x += dx\n",
    "#             pos[child] = (next_x, 1 - vert_gap * (level + 1))\n",
    "#             pos = hierarchy_pos(G, root=child, width=dx, vert_gap=vert_gap, xcenter=next_x, pos=pos, level=level+1)\n",
    "#         return pos\n",
    "\n",
    "#     # Step 1: Parse Mermaid mind map structure\n",
    "#     edges = parse_mermaid_mindmap(mermaid_code)\n",
    "#     if not edges:\n",
    "#         raise ValueError(\"Invalid Mermaid mindmap format.\")\n",
    "\n",
    "#     # Step 2: Create a graph using NetworkX\n",
    "#     G = nx.DiGraph()\n",
    "#     G.add_edges_from(edges)\n",
    "#     root = edges[0][0]  # Take the first node as root\n",
    "#     pos = hierarchy_pos(G, root)\n",
    "\n",
    "#     # Step 3: Generate the mind map image\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\",\n",
    "#             node_size=3000, font_size=4.5, font_weight=\"bold\", alpha=0.8, arrows=False)\n",
    "\n",
    "#     img_buffer = io.BytesIO()\n",
    "#     plt.savefig(img_buffer, format=\"png\", bbox_inches=\"tight\")\n",
    "#     plt.close()\n",
    "#     img_buffer.seek(0)\n",
    "\n",
    "#     # Step 4: Upload the image to GoFile\n",
    "#     files = {\"file\": (\"mindmap.png\", img_buffer, \"image/png\")}\n",
    "#     response = requests.post(\"https://store1.gofile.io/uploadFile\", files=files)\n",
    "#     response.raise_for_status()\n",
    "#     data = response.json()[\"data\"]\n",
    "\n",
    "#     # Construct the direct download link\n",
    "#     server = data[\"servers\"][0]  # Use the first available server\n",
    "#     file_id = data[\"id\"]\n",
    "#     file_name = data[\"name\"]\n",
    "#     return f\"https://{server}.gofile.io/download/web/{file_id}/{file_name}\"\n",
    "\n",
    "# # Example Usage\n",
    "# mermaid_code = \"\"\"\n",
    "# mindmap\n",
    "#   root((Short-term Effects of L-Carnitine for Weight Loss))\n",
    "#     Lipid Metabolism\n",
    "#       Enzyme Interaction\n",
    "#     Energy Expenditure\n",
    "#       Postprandial Effects\n",
    "#       Basal Metabolism\n",
    "#     Plasma Levels\n",
    "#       Dietary Modifications\n",
    "#       Urinary Concentrations\n",
    "#     Gut Microbiota\n",
    "#       Metabolomic Changes\n",
    "#       Microbiota Composition\n",
    "#     Thermogenic Effects\n",
    "#       Thermogenesis\n",
    "#       Pathway Inhibition\n",
    "# \"\"\"\n",
    "# download_link = generate_and_upload_mindmap(mermaid_code)\n",
    "# print(\"Download Link:\", download_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download your mindmap: https://store1.gofile.io/download/web/94e2549e-9c53-4f08-80fb-629f5b20278b/mindmap.png\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "# import textwrap\n",
    "# import requests\n",
    "# import io\n",
    "\n",
    "# def create_and_upload_mindmap(edges, output_file=\"mindmap.png\"):\n",
    "#     \"\"\"\n",
    "#     Creates a mind map using NetworkX and Matplotlib, saves it as an image, and uploads it to Gofile.\n",
    "    \n",
    "#     Args:\n",
    "#         edges (list of tuples): List of (parent, child) relationships.\n",
    "#         output_file (str): Name of the output image file.\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Gofile direct download link.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Create directed graph\n",
    "#     G = nx.DiGraph()\n",
    "#     G.add_edges_from(edges)\n",
    "\n",
    "#     # Layout settings for better spacing\n",
    "#     pos = nx.spring_layout(G, k=0.6, seed=42)  # Adjust k to control spacing\n",
    "\n",
    "#     # Draw nodes and edges\n",
    "#     plt.figure(figsize=(12, 7))  # Increase figure size\n",
    "#     nx.draw(G, pos, with_labels=False, node_size=3000, node_color=\"lightblue\", edge_color=\"gray\", linewidths=1, font_size=10)\n",
    "\n",
    "#     # Add labels with wrapped text\n",
    "#     for node, (x, y) in pos.items():\n",
    "#         wrapped_label = \"\\n\".join(textwrap.wrap(node, width=15))  # Wrap text for readability\n",
    "#         plt.text(x, y, wrapped_label, fontsize=9, ha=\"center\", va=\"center\", bbox=dict(facecolor=\"white\", alpha=0.6, edgecolor=\"none\"))\n",
    "\n",
    "#     # Save image to memory buffer\n",
    "#     buf = io.BytesIO()\n",
    "#     plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "#     plt.close()\n",
    "\n",
    "#     # Upload to Gofile\n",
    "#     buf.seek(0)\n",
    "#     response = requests.post(\"https://store1.gofile.io/uploadFile\", files={\"file\": (\"mindmap.png\", buf, \"image/png\")})\n",
    "#     response.raise_for_status()\n",
    "    \n",
    "#     # Extract download link\n",
    "#     data = response.json()[\"data\"]\n",
    "#     file_id = data[\"id\"]\n",
    "#     server = data[\"servers\"][0]\n",
    "#     download_link = f\"https://{server}.gofile.io/download/web/{file_id}/mindmap.png\"\n",
    "    \n",
    "#     return download_link\n",
    "\n",
    "# # Example usage\n",
    "# edges = [\n",
    "#     (\"mindmap\", \"Main Topic\"),\n",
    "#     (\"Main Topic\", \"First Branch\"),\n",
    "#     (\"Main Topic\", \"Second Branch\"),\n",
    "#     (\"First Branch\", \"Subtopic A\"),\n",
    "#     (\"First Branch\", \"Subtopic B\"),\n",
    "#     (\"Second Branch\", \"Another Idea\"),\n",
    "#     (\"Second Branch\", \"More Thoughts\")\n",
    "# ]\n",
    "\n",
    "# link = create_and_upload_mindmap(edges)\n",
    "# print(\"Download your mindmap:\", link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download your mindmap: https://store1.gofile.io/download/web/c12f3e1c-a066-4716-b7c2-eb08de37d4fb/mindmap.png\n"
     ]
    }
   ],
   "source": [
    "# edges = [\n",
    "#     (\"mindmap\", \"Short-term Effects of L-Carnitine for Weight Loss\"),\n",
    "#     (\"Short-term Effects of L-Carnitine for Weight Loss\", \"Lipid Metabolism\"),\n",
    "#     (\"Lipid Metabolism\", \"Enzyme Interaction\"),\n",
    "#     (\"Short-term Effects of L-Carnitine for Weight Loss\", \"Energy Expenditure\"),\n",
    "#     (\"Energy Expenditure\", \"Postprandial Effects\"),\n",
    "#     (\"Energy Expenditure\", \"Basal Metabolism\"),\n",
    "#     (\"Short-term Effects of L-Carnitine for Weight Loss\", \"Plasma Levels\"),\n",
    "#     (\"Plasma Levels\", \"Dietary Modifications\"),\n",
    "#     (\"Plasma Levels\", \"Urinary Concentrations\"),\n",
    "#     (\"Short-term Effects of L-Carnitine for Weight Loss\", \"Gut Microbiota\"),\n",
    "#     (\"Gut Microbiota\", \"Metabolomic Changes\"),\n",
    "#     (\"Gut Microbiota\", \"Microbiota Composition\"),\n",
    "#     (\"Short-term Effects of L-Carnitine for Weight Loss\", \"Thermogenic Effects\"),\n",
    "#     (\"Thermogenic Effects\", \"Thermogenesis\"),\n",
    "#     (\"Thermogenic Effects\", \"Pathway Inhibition\")\n",
    "# ]\n",
    "\n",
    "# # Generate and upload the mindmap\n",
    "# link = create_and_upload_mindmap(edges)\n",
    "# print(\"Download your mindmap:\", link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mind map saved as mindmap.png\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "\n",
    "# def parse_mermaid_mindmap(mermaid_code):\n",
    "#     \"\"\"Parses Mermaid mindmap syntax into a tree structure.\"\"\"\n",
    "#     lines = mermaid_code.strip().split(\"\\n\")\n",
    "#     edges = []\n",
    "#     parent_stack = []\n",
    "    \n",
    "#     for line in lines:\n",
    "#         indent_level = len(line) - len(line.lstrip())  # Count leading spaces\n",
    "#         node = line.strip()\n",
    "        \n",
    "#         if node:\n",
    "#             while parent_stack and parent_stack[-1][1] >= indent_level:\n",
    "#                 parent_stack.pop()\n",
    "            \n",
    "#             if parent_stack:\n",
    "#                 edges.append((parent_stack[-1][0], node))\n",
    "            \n",
    "#             parent_stack.append((node, indent_level))\n",
    "    \n",
    "#     return edges\n",
    "\n",
    "# def hierarchy_pos(G, root=None, width=1., vert_gap=0.4, xcenter=0.5, pos=None, parent=None, level=0):\n",
    "#     \"\"\"Recursively assigns positions to nodes for a tree layout.\"\"\"\n",
    "#     if pos is None:\n",
    "#         pos = {root: (xcenter, 1)}\n",
    "#     children = list(G.successors(root))\n",
    "#     if not children:\n",
    "#         return pos\n",
    "\n",
    "#     dx = width / max(1, len(children))\n",
    "#     next_x = xcenter - width / 2 - dx / 2\n",
    "#     for child in children:\n",
    "#         next_x += dx\n",
    "#         pos[child] = (next_x, 1 - vert_gap * (level + 1))\n",
    "#         pos = hierarchy_pos(G, root=child, width=dx, vert_gap=vert_gap, xcenter=next_x, pos=pos, parent=root, level=level+1)\n",
    "#     return pos\n",
    "\n",
    "# def create_mindmap(mermaid_code, output_file=\"mindmap.png\"):\n",
    "#     \"\"\"Generates a structured mind map and saves it as an image.\"\"\"\n",
    "#     edges = parse_mermaid_mindmap(mermaid_code)\n",
    "\n",
    "#     if not edges:\n",
    "#         print(\"Error: No valid nodes found in the input.\")\n",
    "#         return\n",
    "\n",
    "#     G = nx.DiGraph()\n",
    "#     G.add_edges_from(edges)\n",
    "\n",
    "#     root = edges[0][0]  # Take first node as root\n",
    "#     pos = hierarchy_pos(G, root)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", \n",
    "#             node_size=3000, font_size=10, font_weight=\"bold\", alpha=0.8, arrows=False)\n",
    "\n",
    "#     plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "#     plt.close()\n",
    "#     print(f\"Mind map saved as {output_file}\")\n",
    "\n",
    "# # Example Mermaid-style input\n",
    "# mermaid_code = \"\"\"\n",
    "# mindmap\n",
    "#   Main Topic\n",
    "#     First Branch\n",
    "#       Subtopic A\n",
    "#       Subtopic B\n",
    "#     Second Branch\n",
    "#       Another Idea\n",
    "#       More Thoughts\n",
    "# \"\"\"\n",
    "\n",
    "# create_mindmap(mermaid_code, \"mindmap.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
